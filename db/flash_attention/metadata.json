{
    "title": "Flash Attention in Triton",
    "category": "Transformers",
    "tags": [
        "flash-attention",
        "triton",
        "cuda",
        "optimization",
        "memory-efficient"
    ],
    "difficulty": "Hard"
}