{
    "title": "Multi-Head Attention (PyTorch)",
    "category": "Transformers",
    "framework": "pytorch",
    "tags": [
        "pytorch",
        "attention",
        "multihead-attention",
        "transformer",
        "masking"
    ],
    "difficulty": "Medium",
    "relevant_questions": [
        "softmax_numpy",
        "linear_forward_backward_numpy",
        "layernorm_forward_backward_numpy"
    ]
}