---
description: "MLBites: generate a new ML question package under ./db/<question_slug> (strict metadata schema, consistent docs, interview-friendly code)"
globs:
  - "db/**"
alwaysApply: false
---

# MLBites Question Package Rule (STRICT)

When the user asks to create a new question (e.g., "Question for implementation of X or Y"):
Create a new folder at:
  ./db/<question_slug>/

It MUST contain exactly these files:
- metadata.json
- question.md
- solution.md
- solution.py
- starting_point.py
- tests.py

Use lower_snake_case for <question_slug>, e.g. "linear_regression", "softmax_cross_entropy".

---

## metadata.json (STRICT SCHEMA)

Write valid JSON (no comments, no trailing commas). It MUST match this exact structure:

{
  "title": "<Human readable title>",
  "category": "<Category name>",
  "tags": ["tag1", "tag2", ...],
  "difficulty": "Easy" | "Medium" | "Hard",
  "relevant_questions": ["other_question_slug_1", "other_question_slug_2"]
}

Rules:
- Only these 5 keys. No extras.
- "title": short and specific.
- "category": one of your existing categories (if unclear, default to "Basics").
- "tags": 3–8 lowercase tags, kebab-case or simple words; be consistent.
  - MUST include "numpy" if the question allows/uses NumPy.
  - MUST include "pytorch" if the question allows/uses PyTorch / torch.
- "difficulty": choose based on expected 20–30 minute implementation.
- "relevant_questions": list of related question slugs (folders under ./db/) to help build future "recommended next" flows.
  - This field will NOT be shown on the website for now.
  - Use an empty list [] if none.
  - Prefer 0–5 items, and only include slugs that already exist in the repo.

---

## question.md (STRICT STRUCTURE)

Use this exact section order and headings:

# <Title>

## Problem
Explain the goal in plain English.

## Task
State exactly what to implement (function/class) and what it should accomplish.

## Function Signature
Provide the exact Python signature to implement.
This signature MUST match starting_point.py and solution.py.

## Inputs and Outputs
Bullets describing:
- inputs: types/shapes/ranges
- outputs: types/shapes and meaning

## Constraints
- Must be solvable in 20–30 minutes.
- Interview-friendly: avoid heavy boilerplate.
- Allowed libs: default to Python stdlib only.
  If needed, allow numpy OR torch, but only if the task explicitly requires it.

## Examples
Provide at least 2 deterministic examples with expected outputs.

---

## starting_point.py (STRICT)

- Contains ONLY:
  - imports (minimal)
  - function/class signature
  - docstring describing inputs/outputs and TODO steps
  - Not implemented body: raise NotImplementedError
- Must NOT contain the final logic.
- Must be runnable (no syntax errors).

Template:

```python
from __future__ import annotations

def <name>(...):
    \"\"\"<brief docstring>

    Args:
        ...

    Returns:
        ...
    \"\"\"
    # TODO: implement
    raise NotImplementedError


⸻

## tests.py (STRICT)

Each question package MUST include `tests.py` so we can automatically verify candidate solutions.

**Goal**: deterministic, fast checks that validate correctness (including edge cases), plus optional numerical checks (e.g., finite-difference gradient checks) when relevant.

`tests.py` MUST:
- Be deterministic (seed RNGs).
- Contain helper functions inside the same file (no extra files).
- Avoid network / filesystem writes.
- Contain NO `print()`s by default (the CLI runner will report pass/fail).
- Fail by raising `AssertionError` with a helpful message.

### Required interface

`tests.py` MUST define:

```python
from __future__ import annotations

from types import ModuleType

def run_tests(candidate: ModuleType) -> None:
    \"\"\"Run all tests against a candidate solution module.

    Args:
        candidate: a Python module that defines the required function(s)/class(es)
            for this question (same signature as in starting_point.py).

    Raises:
        AssertionError: if any test fails.
    \"\"\"
    # TODO: implement tests
    raise NotImplementedError
```

Notes:
- `candidate` will typically be the user's code (or `solution.py`) loaded as a module.
- If a question requires multiple functions (e.g., forward/backward), `run_tests` should validate all of them.

⸻

solution.py (INTERVIEW-FRIENDLY + MINIMAL)
	•	Minimal reference implementation, readable in one sitting.
	•	Deterministic, test-friendly:
	•	no prints
	•	no input()
	•	Use clear variable names and straightforward steps.
	•	Include type hints and a concise docstring.
	•	Avoid unnecessary abstractions/classes.
	•	Prefer stdlib; allow numpy/torch only if the question requires it.

⸻

solution.md (STRICT STRUCTURE)

Approach

High-level reasoning, 5–12 bullets.

Correctness

Explain why it works (reference edge cases).

Complexity

Time and space complexity.

Common Pitfalls

3–6 bullets.

Keep it short and practical.

⸻

Final self-check (MUST DO)

Before finishing:
	1.	Ensure ./db/<question_slug>/ contains all 6 files.
	2.	Ensure metadata.json has ONLY the 5 keys, a valid difficulty value, and a list for relevant_questions.
        Also ensure "tags" includes "numpy"/"pytorch" when those libs are allowed/used.
	3.	Ensure the function/class signature matches in:
question.md, starting_point.py, solution.py
	4.	Ensure examples are deterministic and consistent with the solution.
	5.	Ensure the solution is realistically doable in 20–30 minutes.
	6.	Ensure `tests.py` runs and validates `solution.py` (deterministically).